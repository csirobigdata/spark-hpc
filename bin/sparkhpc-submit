#!/usr/bin/python

import argparse
from string import Template
import os
from subprocess import Popen, PIPE

QSUB_TEMPLATE='''
#!/bin/bash
#PBS -N ${name}
#PBS -l nodes=${num_executors}:ppn=1,vmem=${vmem}GB
#PBS -l walltime=${walltime}
#PBS -j oe
#PBS -o ${output}
#PBS -V

set -e

export SPARKHPC_DRIVER_CLASSPATH=${driver_classpath} 
export SPARKHPC_EXECUTOR_CLASSPATH=${executor_classpath}
export LD_LIBRARY_PATH=${spark_library_path}
export SPARK_JAVA_OPTS="${spark_java_opts}"
export SPARKHPC_EXECUTOR_MEM="${executor_mem}G"
export SPARKHPC_DRIVER_MEM="${driver_mem}G"

$${SPARKHPC_HOME}/bin/spark-hpc.sh --verbose ${clazz} ${args} 
'''

CWD = os.getcwd()

def resolve_paths(l):
    return map(lambda p: os.path.abspath(p),filter(lambda p:len(p) >0,l))

def human2bytes(s):
    """
    >>> human2bytes('1M')
    1048576
    >>> human2bytes('1G')
    1073741824
    """
    symbols = ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
    letter = s[-1:].strip().upper()
    num = s[:-1]
    assert num.isdigit() and letter in symbols
    num = float(num)
    prefix = {symbols[0]:1}
    for i, s in enumerate(symbols[1:]):
        prefix[s] = 1 << (i+1)*10
    return int(num * prefix[letter])

def validate_conf(conf):
    non_spark_conf = filter(lambda x:not x.startswith("spark."), conf)
    if len(non_spark_conf) > 0 :
        print("Invalid spark options: %s" % non_spark_conf)
        sys.exit(1)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-c','--class', help="the main class to run", dest="clazz" )
    parser.add_argument('--jars', help="jars to add to classpath", default="")
    parser.add_argument('--driver-memory', help="memory per node in GB (default=1)", default="1")
    parser.add_argument('--driver-java-options', help="Extra Java options to pass to the driver vm, e.g: ' -Dfoo=bar -Dxxx-zzz'. Please note the \"'\" and the leading space! ", default="")
    parser.add_argument('--driver-library-path', help="Extra library path entries to pass to the driver.", default="")
    parser.add_argument('--driver-class-path', help="Extra class path entries to pass to the driver.", default="")
    parser.add_argument('--executor-memory', help="memory per node in GB", default="1")
    parser.add_argument('--conf', help="sparkConfigOptions", default=[], action='append') 
    misc_options = parser.add_argument_group('misc options' )
    misc_options.add_argument('--dry-run', help="Print out the pbs script but do not run", action='store_true', default=False)
    misc_options.add_argument('-v', '--verbose', help="Print verbose info", action='store_true', default=False)
    misc_options.add_argument('-l', '--local', help="Do not submit to the cluster - run locally", action='store_true', default=False)
    pbs_group  = parser.add_argument_group('PBS Only', 'Options for PBS')
    pbs_group.add_argument('--num-executors', help="number of nodes (default = 1) ", default="1")
    pbs_group.add_argument('--executor-cores', help="number of cores pre node", default="1")
    pbs_group.add_argument('-o', '--output', help="Output file")
    pbs_group.add_argument('-w', '--walltime', help="walltime", default="10:00")
    parser.add_argument('appJar', help="the main class to run")
    parser.add_argument('args', nargs=argparse.REMAINDER)
    args = parser.parse_args()
    if args.verbose:
        print("Parsed arguments: %s" % args)

    validate_conf(args.conf) 
    jobname = args.clazz.split(".")[-1]
    vmem_in_gb = int(args.num_executors) * int(args.executor_memory) + int(args.driver_memory)
    vmem = vmem_in_gb 
    driver_classpath = ":".join(resolve_paths([args.appJar] + args.jars.split(",") + args.driver_class_path.split(":")))
    executor_classpath = ":".join(resolve_paths([args.appJar] + args.jars.split(",")))
    spark_library_path = ":".join(resolve_paths(args.driver_library_path.split(":")))
    output = os.path.abspath(args.output if args.output is not None else "%s.oe" % jobname)
    qsub_script = Template(QSUB_TEMPLATE).substitute(name=jobname, clazz=args.clazz, appjar=args.appJar,
        driver_classpath = driver_classpath, executor_classpath = executor_classpath,  walltime = args.walltime, num_executors = args.num_executors,
	vmem = vmem, driver_mem = args.driver_memory, executor_mem = args.executor_memory, 
	spark_java_opts = args.driver_java_options.strip(), spark_library_path=spark_library_path, 
        output = output, args = " ".join(args.args))
    if args.dry_run or args.verbose:
    	print(qsub_script)
    exec_prog = 'qsub' if not args.local else 'bash'
    if not args.dry_run:
        p = Popen([exec_prog,'-'],stdin=PIPE,shell=True)
        p.communicate(qsub_script) 	
        p.stdin.close()
        p.wait()
