#!/usr/bin/python

import argparse
from string import Template
import os
from subprocess import Popen, PIPE

QSUB_TEMPLATE='''
#!/bin/bash
#PBS -N ${name}
#PBS -l nodes=${num_executors}:ppn=1,vmem=${vmem}GB
#PBS -l walltime=${walltime}
#PBS -j oe
#PBS -o ${output}
#PBS -v SPARKHPC_ROOT

set -e

. ${SPARKHPC_ROOT}/load_spark.sh
. ${SPARKHPC_ROOT}/load_sparkhpc.sh

export SPARK_CLASSPATH=${spark_classpath} 
export LD_LIBRARY_PATH=${spark_library_path}
export SPARK_JAVA_OPTS="${spark_java_opts}"
export SPARK_MEM="${executor_mem}G"
export SPARKHPC_DRIVER_MEM="${driver_mem}G"

spark-hpc.sh --url-env-var ${clazz} ${args} 
'''

CWD = os.getcwd()

def resolve_paths(l):
    return map(lambda p: os.path.abspath(p),l)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--num-executors', help="number of nodes (default = 1) ", default="1")
    parser.add_argument('-cn', '--cores-per-node', help="number of cores pre node", default="1")
    parser.add_argument('-dm', '--driver-memory', help="memory per node in GB", default="1")
    parser.add_argument('-djo', '--driver-java-options', help="Extra Java options to pass to the driver.", default="")
    parser.add_argument('-dlp', '--driver-library-path', help="Extra library path entries to pass to the driver.", default="")
    parser.add_argument('-dcp', '--driver-class-path', help="Extra class path entries to pass to the driver.", default="")
    parser.add_argument('-em', '--executor-memory', help="memory per node in GB", default="1")
    parser.add_argument('-w', '--walltime', help="walltime", default="10:00")
    parser.add_argument('-o', '--output', help="Output file")
    parser.add_argument('-d','--debug', dest='debug', action='store_true', help="turn on loging debug info")
    parser.add_argument('-c','--clazz', help="the main class to run")
    parser.add_argument('-j','--jars', help="jars to add to classpath", default="")
    parser.add_argument('appJar', help="the main class to run")
    parser.add_argument('args', nargs=argparse.REMAINDER)
    args = parser.parse_args()
    print(args)
    jobname = args.clazz.split(".")[-1]
    vmem = int(args.num_executors) * int(args.executor_memory) + int(args.driver_memory)
    spark_classpath = ":".join(resolve_paths([args.appJar] + args.jars.split(",") + args.driver_class_path.split(":")))
    spark_library_path = ":".join(resolve_paths(args.driver_library_path.split(":")))
    output = os.path.abspath(args.output if args.output is not None else "%s.oe" % jobname)
    print("From template")
    qsub_script = Template(QSUB_TEMPLATE).substitute(name=jobname, clazz=args.clazz, appjar=args.appJar,
        spark_classpath = spark_classpath, walltime = args.walltime, num_executors = args.num_executors,
	vmem = vmem, driver_mem = args.driver_memory, executor_mem = args.executor_memory, 
	spark_java_opts = args.driver_java_options.strip(), spark_library_path=spark_library_path, 
        SPARKHPC_ROOT = os.environ["SPARKHPC_ROOT"], output = output, args = " ".join(args.args))
    print(qsub_script)
    p = Popen(['qsub','-'],stdin=PIPE,shell=True)
    p.communicate(qsub_script) 	
    p.stdin.close()
    p.wait()
